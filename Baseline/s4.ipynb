{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "from s4 import S4Block as S4 \n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, feature_num]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1)].unsqueeze(0)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "\n",
    "class S4ModelForRUL(nn.Module):\n",
    "    def __init__(self, d_input, d_model=512, n_layers=4, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout, max_len=max_len)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "        self.bn_encoder = nn.BatchNorm1d(max_len) \n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(S4(d_model, dropout=dropout, transposed=True))\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "       \n",
    "        src = self.encoder(src)  # [batch_size, seq_len, d_input] -> [batch_size, seq_len, d_model]\n",
    "          # Apply tanh activation function\n",
    "        src = self.bn_encoder(src)  \n",
    "        src = F.tanh(src)\n",
    "        #src = self.pos_encoder(src)\n",
    "        src = src.transpose(1, 2)  # S4 expects [batch_size, d_model, seq_len]\n",
    "        for layer in self.s4_layers:\n",
    "            src, _ = layer(src)  # We ignore the state output here\n",
    "        src = src.transpose(1, 2)  # Back to [batch_size, seq_len, d_model]\n",
    "        src = self.dropout(src)\n",
    "        output = self.decoder(src) \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optimizer(model, lr, weight_decay, epochs):\n",
    "    \"\"\"\n",
    "    S4 requires a specific optimizer setup.\n",
    "\n",
    "    The S4 layer (A, B, C, dt) parameters typically\n",
    "    require a smaller learning rate (typically 0.001), with no weight decay.\n",
    "\n",
    "    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01)\n",
    "    and weight decay (if desired).\n",
    "    \"\"\"\n",
    "\n",
    "    # All parameters in the model\n",
    "    all_parameters = list(model.parameters())\n",
    "\n",
    "    # General parameters don't contain the special _optim key\n",
    "    params = [p for p in all_parameters if not hasattr(p, \"_optim\")]\n",
    "\n",
    "    # Create an optimizer with the general parameters\n",
    "    optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Add parameters with special hyperparameters\n",
    "    hps = [getattr(p, \"_optim\") for p in all_parameters if hasattr(p, \"_optim\")]\n",
    "    hps = [\n",
    "        dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n",
    "    ]  # Unique dicts\n",
    "    for hp in hps:\n",
    "        params = [p for p in all_parameters if getattr(p, \"_optim\", None) == hp]\n",
    "        optimizer.add_param_group(\n",
    "            {\"params\": params, **hp}\n",
    "        )\n",
    "\n",
    "    # Create a lr scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=0.2)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    # Print optimizer info\n",
    "    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        group_hps = {k: g.get(k, None) for k in keys}\n",
    "        print(' | '.join([\n",
    "            f\"Optimizer group {i}\",\n",
    "            f\"{len(g['params'])} tensors\",\n",
    "        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class load_data(Dataset):\n",
    "    \"\"\"\n",
    "    root = new | old\n",
    "    \"\"\"\n",
    "    def __init__(self, name, seq_len, root='new') -> None:\n",
    "        super().__init__()\n",
    "        data_root = \"data/units/\"\n",
    "        if root == 'old':\n",
    "            label_root = \"data/labels/\"\n",
    "        elif root == 'new':\n",
    "            label_root = \"data/new_labels/\"\n",
    "        else:\n",
    "            raise RuntimeError(\"got invalid parameter root='{}'\".format(root))\n",
    "        raw = np.loadtxt(data_root+name)[:,2:]\n",
    "        lbl = np.loadtxt(label_root+name)/Rc\n",
    "        l = len(lbl)\n",
    "        if l<seq_len:\n",
    "            raise RuntimeError(\"seq_len {} is too big for file '{}' with length {}\".format(seq_len, name, l))\n",
    "        raw, lbl = torch.tensor(raw, dtype=torch.float), torch.tensor(lbl, dtype=torch.float)\n",
    "        lbl_pad_0 = [torch.ones([seq_len-i-1]) for i in range(seq_len-1)] \n",
    "        data_pad_0 = [torch.zeros([seq_len-i-1,24]) for i in range(seq_len-1)]\n",
    "        lbl_pad_1 = [torch.zeros([i+1]) for i in range(seq_len-1)] \n",
    "        data_pad_1 = [torch.zeros([i+1,24]) for i in range(seq_len-1)]\n",
    "        self.data = [torch.cat([data_pad_0[i],raw[:i+1]],0) for i in range(seq_len-1)] \n",
    "        self.data += [raw[i-seq_len+1:i+1] for i in range(seq_len-1, l)]\n",
    "        self.data += [torch.cat([raw[l-seq_len+i+1:], data_pad_1[i]],0) for i in range(seq_len-1)]\n",
    "        self.label = [torch.cat([lbl_pad_0[i],lbl[:i+1]],0) for i in range(seq_len-1)] \n",
    "        self.label += [lbl[i-seq_len+1:i+1] for i in range(seq_len-1, l)]\n",
    "        self.label += [torch.cat([lbl[l-seq_len+i+1:], lbl_pad_1[i]],0) for i in range(seq_len-1)]\n",
    "        self.padding = [torch.cat([torch.ones(seq_len-i-1), torch.zeros(i+1)],0) for i in range(seq_len-1)]   # 1 for ingore\n",
    "        self.padding += [torch.zeros(seq_len) for i in range(seq_len-1, l)]\n",
    "        self.padding += [torch.cat([torch.zeros(seq_len-i-1), torch.ones(i+1)],0) for i in range(seq_len-1)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index], self.padding[index]\n",
    "\n",
    "\n",
    "class load_all_data(Dataset):\n",
    "    \"\"\"\n",
    "    root: new | old\n",
    "    name: LIST of txt files to collect \n",
    "    \"\"\"\n",
    "    def __init__(self, name, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        data_root = \"data/units/\"\n",
    "        label_root = \"data/new_labels/\"\n",
    "        lis = os.listdir(data_root)\n",
    "        data_list = [i for i in lis if i in name]\n",
    "        self.data, self.label, self.padding = [], [], []\n",
    "        for n in data_list:\n",
    "            raw = np.loadtxt(data_root+n)[:,2:]\n",
    "            lbl = np.loadtxt(label_root+n)/Rc\n",
    "            l = len(lbl)\n",
    "            if l<seq_len:\n",
    "                raise RuntimeError(\"seq_len {} is too big for file '{}' with length {}\".format(seq_len, n, l))\n",
    "            raw, lbl = torch.tensor(raw, dtype=torch.float), torch.tensor(lbl, dtype=torch.float)\n",
    "            lbl_pad_0 = [torch.ones([seq_len-i-1]) for i in range(seq_len-1)] \n",
    "            data_pad_0 = [torch.zeros([seq_len-i-1,24]) for i in range(seq_len-1)]\n",
    "            lbl_pad_1 = [torch.zeros([i+1]) for i in range(seq_len-1)] \n",
    "            data_pad_1 = [torch.zeros([i+1,24]) for i in range(seq_len-1)]\n",
    "            self.data += [torch.cat([data_pad_0[i],raw[:i+1]],0) for i in range(seq_len-1)] \n",
    "            self.data += [raw[i-seq_len+1:i+1] for i in range(seq_len-1, l)]\n",
    "            self.data += [torch.cat([raw[l-seq_len+i+1:], data_pad_1[i]],0) for i in range(seq_len-1)]\n",
    "            self.label += [torch.cat([lbl_pad_0[i],lbl[:i+1]],0) for i in range(seq_len-1)] \n",
    "            self.label += [lbl[i-seq_len+1:i+1] for i in range(seq_len-1, l)]\n",
    "            self.label += [torch.cat([lbl[l-seq_len+i+1:], lbl_pad_1[i]],0) for i in range(seq_len-1)]\n",
    "            self.padding += [torch.cat([torch.ones(seq_len-i-1), torch.zeros(i+1)],0) for i in range(seq_len-1)]   # 1 for ingore\n",
    "            self.padding += [torch.zeros(seq_len) for i in range(seq_len-1, l)]\n",
    "            self.padding += [torch.cat([torch.zeros(seq_len-i-1), torch.ones(i+1)],0) for i in range(seq_len-1)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index], self.padding[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'FD001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = np.loadtxt(\"save/\"+name+\"/train\"+name+\".txt\", dtype=str).tolist()\n",
    "val = np.loadtxt(\"save/\"+name+\"/valid\"+name+\".txt\", dtype=str).tolist()\n",
    "ts = np.loadtxt(\"save/\"+name+\"/test\"+name+\".txt\", dtype=str).tolist()\n",
    "\n",
    "target = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:71isauf3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ValLoss</td><td>█▂▂▂▃▂▂▂▂▃▂▂▂▂▆▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>optimizer</td><td>████████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>scheduler</td><td>████████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>99</td></tr><tr><td>Loss</td><td>0.00566</td></tr><tr><td>ValLoss</td><td>9.43151</td></tr><tr><td>optimizer</td><td>0.0</td></tr><tr><td>scheduler</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">s4-rul</strong> at: <a href='https://wandb.ai/djbd/S4RUL/runs/71isauf3' target=\"_blank\">https://wandb.ai/djbd/S4RUL/runs/71isauf3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240212_114824-71isauf3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:71isauf3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/djoubaud/RUL/RUL/Baseline/wandb/run-20240212_115430-3w93orzl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djbd/S4RUL/runs/3w93orzl' target=\"_blank\">s4-rul</a></strong> to <a href='https://wandb.ai/djbd/S4RUL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djbd/S4RUL' target=\"_blank\">https://wandb.ai/djbd/S4RUL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djbd/S4RUL/runs/3w93orzl' target=\"_blank\">https://wandb.ai/djbd/S4RUL/runs/3w93orzl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer group 0 | 9 tensors | weight_decay 1e-05\n",
      "Optimizer group 1 | 1 tensors | weight_decay 1e-05\n",
      "Optimizer group 2 | 5 tensors | weight_decay 0.0\n"
     ]
    }
   ],
   "source": [
    "# add wandb\n",
    "import wandb\n",
    "\n",
    "wandb.login(key = '89972c25af0c49a4e2e1b8663778daedd960634a')\n",
    "\n",
    "wandb.init(project=\"S4RUL\", name=\"s4-rul\")\n",
    "#login to wandb\n",
    "\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "d_input = 24  \n",
    "seq_len = 70\n",
    "Rc = 130\n",
    "model = S4ModelForRUL(d_input=d_input, d_model=512, n_layers=1, dropout=0.1, max_len=seq_len)\n",
    "model.to(device)\n",
    "Loss = nn.MSELoss()\n",
    "Loss.to(device)\n",
    "\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# sch = torch.optim.lr_scheduler.StepLR(opt, 50, 0.5)\n",
    "epochs = 100\n",
    "opt, sch = setup_optimizer(\n",
    "    model, lr=0.02, weight_decay=1e-5, epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.796982077315349, Val loss: 13.255811116205543\n",
      "Epoch: 1, Loss: 0.016908917355829396, Val loss: 14.015228992744243\n",
      "Epoch: 2, Loss: 0.014301676888371239, Val loss: 10.913010650616386\n",
      "Epoch: 3, Loss: 0.012068757762175959, Val loss: 10.530173100835952\n",
      "Epoch: 4, Loss: 0.011379879257465536, Val loss: 13.712009359513047\n",
      "Epoch: 5, Loss: 0.010862489709177532, Val loss: 14.371095176020935\n",
      "Epoch: 6, Loss: 0.010716358646498742, Val loss: 11.13789976453378\n",
      "Epoch: 7, Loss: 0.010219217089281694, Val loss: 9.976751508418205\n",
      "Epoch: 8, Loss: 0.009963445941212814, Val loss: 11.453026950807157\n",
      "Epoch: 9, Loss: 0.010084876762325497, Val loss: 12.797629781812121\n",
      "Epoch: 10, Loss: 0.009569004426609623, Val loss: 10.58275069349566\n",
      "Epoch: 11, Loss: 0.009135505835827742, Val loss: 11.48179375436669\n",
      "Epoch: 12, Loss: 0.009264894635290713, Val loss: 10.0178961679792\n",
      "Epoch: 13, Loss: 0.009009217255044024, Val loss: 9.826691575017662\n",
      "Epoch: 14, Loss: 0.009131206750139795, Val loss: 11.4946812198553\n",
      "Epoch: 15, Loss: 0.00869339651025429, Val loss: 9.604611468204512\n",
      "Epoch: 16, Loss: 0.008815286737690503, Val loss: 10.916339523104305\n",
      "Epoch: 17, Loss: 0.008632620802853961, Val loss: 11.393038964165266\n",
      "Epoch: 18, Loss: 0.008526300209121325, Val loss: 10.229273436592107\n",
      "Epoch: 19, Loss: 0.0082429971898327, Val loss: 9.210538328727852\n",
      "Epoch: 20, Loss: 0.008336967357585358, Val loss: 11.973790844696994\n",
      "Epoch: 21, Loss: 0.008310161184871922, Val loss: 10.47998072891945\n",
      "Epoch: 22, Loss: 0.008124413815125622, Val loss: 9.781134682536752\n",
      "Epoch: 23, Loss: 0.007959052438948405, Val loss: 10.60138203620478\n",
      "Epoch: 24, Loss: 0.008105363070687934, Val loss: 10.978815185777433\n",
      "Epoch: 25, Loss: 0.00813266051920584, Val loss: 11.145713832700105\n",
      "Epoch: 26, Loss: 0.008067290295494368, Val loss: 9.194413402351438\n",
      "Epoch: 27, Loss: 0.007995716736626786, Val loss: 12.302118976981355\n",
      "Epoch: 28, Loss: 0.007972982422624892, Val loss: 12.537746657938378\n",
      "Epoch: 29, Loss: 0.007848660952436763, Val loss: 10.562065658136916\n",
      "Epoch: 30, Loss: 0.007681087757511115, Val loss: 9.250919817718412\n",
      "Epoch: 31, Loss: 0.007773925789405365, Val loss: 9.44826336650479\n",
      "Epoch: 32, Loss: 0.007883801943043599, Val loss: 9.822058167320451\n",
      "Epoch: 33, Loss: 0.00754999802951273, Val loss: 9.518615977558017\n",
      "Epoch: 34, Loss: 0.007685034601898813, Val loss: 8.967756712330528\n",
      "Epoch: 35, Loss: 0.007403879870056502, Val loss: 8.925476283536366\n",
      "Epoch: 36, Loss: 0.007293978617904154, Val loss: 9.8181905975427\n",
      "Epoch: 37, Loss: 0.007498080379682014, Val loss: 9.291272866904409\n",
      "Epoch: 38, Loss: 0.007326685474254191, Val loss: 10.714048460929272\n",
      "Epoch: 39, Loss: 0.007133499593348117, Val loss: 10.365659964023322\n",
      "Epoch: 40, Loss: 0.0070563946989037705, Val loss: 9.286381906637002\n",
      "Epoch: 41, Loss: 0.007136974093579763, Val loss: 11.54032542141469\n",
      "Epoch: 42, Loss: 0.006910939269850182, Val loss: 10.023421374297481\n",
      "Epoch: 43, Loss: 0.0068719468639559435, Val loss: 10.782127195819882\n",
      "Epoch: 44, Loss: 0.006668839128562124, Val loss: 9.395334985397955\n",
      "Epoch: 45, Loss: 0.006701170199399663, Val loss: 8.973697298650986\n",
      "Epoch: 46, Loss: 0.006461314967128675, Val loss: 10.692304131900835\n",
      "Epoch: 47, Loss: 0.006278602897214728, Val loss: 10.218063927032736\n",
      "Epoch: 48, Loss: 0.006079729832079564, Val loss: 9.23241247933851\n",
      "Epoch: 49, Loss: 0.006077539855362596, Val loss: 10.505785869521757\n",
      "Epoch: 50, Loss: 0.005771993659436703, Val loss: 9.28817525446402\n",
      "Epoch: 51, Loss: 0.005773648253100849, Val loss: 9.706475935237096\n",
      "Epoch: 52, Loss: 0.00558170914725476, Val loss: 10.66273454344281\n",
      "Epoch: 53, Loss: 0.005292105085745051, Val loss: 11.344223888690443\n",
      "Epoch: 54, Loss: 0.005072435069942495, Val loss: 9.592831934944238\n",
      "Epoch: 55, Loss: 0.005096506258170749, Val loss: 9.780429132066846\n",
      "Epoch: 56, Loss: 0.004771114268805832, Val loss: 11.015656801909117\n",
      "Epoch: 57, Loss: 0.004704158998572746, Val loss: 10.196622812842206\n",
      "Epoch: 58, Loss: 0.004520596522598754, Val loss: 10.583398769194158\n",
      "Epoch: 59, Loss: 0.0043509607281963765, Val loss: 9.968884654772294\n",
      "Epoch: 60, Loss: 0.004144079921880385, Val loss: 10.598112866760081\n",
      "Epoch: 61, Loss: 0.004140483923382251, Val loss: 10.677847613031544\n",
      "Epoch: 62, Loss: 0.0040081111600974925, Val loss: 10.705347323570095\n",
      "Epoch: 63, Loss: 0.003861376647585751, Val loss: 10.473929618334378\n",
      "Epoch: 64, Loss: 0.003803291279394683, Val loss: 10.750923944537336\n",
      "Epoch: 65, Loss: 0.0036807289988910023, Val loss: 10.158342207677997\n",
      "Epoch: 66, Loss: 0.0035730540953777933, Val loss: 10.780904883530736\n",
      "Epoch: 67, Loss: 0.0034640625628919617, Val loss: 10.546150856425218\n",
      "Epoch: 68, Loss: 0.0033947067379297034, Val loss: 11.293272417266648\n",
      "Epoch: 69, Loss: 0.0033680593116661986, Val loss: 11.316929800534316\n",
      "Epoch: 70, Loss: 0.0032618964481091984, Val loss: 10.798846690266894\n",
      "Epoch: 71, Loss: 0.0031410027688646034, Val loss: 10.89299725711035\n",
      "Epoch: 72, Loss: 0.0030806806076922127, Val loss: 10.790264455512068\n",
      "Epoch: 73, Loss: 0.003011769490832513, Val loss: 10.552222385905074\n",
      "Epoch: 74, Loss: 0.002966940085202254, Val loss: 10.963817420505732\n",
      "Epoch: 75, Loss: 0.002897050472391719, Val loss: 10.978607094503417\n",
      "Epoch: 76, Loss: 0.002835054796132435, Val loss: 11.365221919297328\n",
      "Epoch: 77, Loss: 0.0028156741833742205, Val loss: 11.111391349420089\n",
      "Epoch: 78, Loss: 0.00272333953369132, Val loss: 10.888464771930671\n",
      "Epoch: 79, Loss: 0.0027075273321463247, Val loss: 11.206444833883095\n",
      "Epoch: 80, Loss: 0.0026292729197774788, Val loss: 10.7304405064071\n",
      "Epoch: 81, Loss: 0.0026035535013982773, Val loss: 10.987516764492634\n",
      "Epoch: 82, Loss: 0.002564986510126776, Val loss: 10.93959850916688\n",
      "Epoch: 83, Loss: 0.0025200792808538755, Val loss: 11.104563394412372\n",
      "Epoch: 84, Loss: 0.0024890223631282918, Val loss: 11.167982642021398\n",
      "Epoch: 85, Loss: 0.002455150435609441, Val loss: 10.972770484033552\n",
      "Epoch: 86, Loss: 0.0024335959169548005, Val loss: 11.077959988738248\n",
      "Epoch: 87, Loss: 0.0024119314998765854, Val loss: 11.10632276041523\n",
      "Epoch: 88, Loss: 0.0023753721372388, Val loss: 11.078516470679054\n",
      "Epoch: 89, Loss: 0.0023632721292685616, Val loss: 11.276417935002607\n",
      "Epoch: 90, Loss: 0.002340014658968996, Val loss: 11.025957889000317\n",
      "Epoch: 91, Loss: 0.0023128906732135628, Val loss: 11.140659148023671\n",
      "Epoch: 92, Loss: 0.0023086139085903967, Val loss: 11.001327563592076\n",
      "Epoch: 93, Loss: 0.002309412249470935, Val loss: 11.154157758313255\n",
      "Epoch: 94, Loss: 0.002292629494174459, Val loss: 11.110847192649066\n",
      "Epoch: 95, Loss: 0.0022816504745089726, Val loss: 11.177671652219432\n",
      "Epoch: 96, Loss: 0.0022740771072111217, Val loss: 11.109120264874754\n",
      "Epoch: 97, Loss: 0.0022676072847979093, Val loss: 11.083209431034355\n",
      "Epoch: 98, Loss: 0.0022622580101990418, Val loss: 11.073539495598766\n",
      "Epoch: 99, Loss: 0.0022598819797732743, Val loss: 11.095327854006783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.925476283536366"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "def get_score(pred, truth):\n",
    "    \"\"\"input must be tensors!\"\"\"\n",
    "    x = pred-truth\n",
    "    score1 = torch.tensor([torch.exp(-i/13)-1 for i in x if i<0])\n",
    "    score2 = torch.tensor([torch.exp(i/10)-1 for i in x if i>=0])\n",
    "    return int(torch.sum(score1)+torch.sum(score2))\n",
    "\n",
    "def get_pred_result(data_len, out, lb):\n",
    "    pred_sum, pred_cnt = torch.zeros(800), torch.zeros(800)\n",
    "    for j in range(data_len):\n",
    "        if j < seq_len-1:\n",
    "            pred_sum[:j+1] += out[j, -(j+1):]\n",
    "            pred_cnt[:j+1] += 1\n",
    "        elif j <= data_len-seq_len:\n",
    "            pred_sum[j-seq_len+1:j+1] += out[j]\n",
    "            pred_cnt[j-seq_len+1:j+1] += 1\n",
    "        else:\n",
    "            pred_sum[data_len-seq_len+1-(data_len-j):data_len-seq_len+1] += out[j, :(data_len-j)]\n",
    "            pred_cnt[data_len-seq_len+1-(data_len-j):data_len-seq_len+1] += 1\n",
    "    truth = torch.tensor([lb[j,-1] for j in range(len(lb[0])-seq_len+1)], dtype=torch.float)\n",
    "    \n",
    "    pred_sum, pred_cnt = pred_sum[:data_len-seq_len+1], pred_cnt[:data_len-seq_len+1]\n",
    "    pred2 = pred_sum/pred_cnt\n",
    "    pred2 *= Rc\n",
    "    truth *= Rc\n",
    "    return truth, pred2 \n",
    "\n",
    "def train(data, model, loss_function, optimizer, seq_len, epochs, device, name):\n",
    "    min_rmse = float('inf')\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        random.shuffle(data)\n",
    "        train_data = load_all_data(data, seq_len=seq_len)  # Ensure this returns a dataset compatible with DataLoader\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "       \n",
    "        for train_data, train_label, train_padding in train_loader:\n",
    "            \n",
    "            train_data, train_label = train_data.to(device), train_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data).squeeze()  # Adjusted to pass only train_data\n",
    "            \n",
    "            \n",
    "            loss = loss_function(output, train_label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "       \n",
    "           \n",
    "        rmse = validate(model, seq_len, device, target, score = False) \n",
    "        wandb.log({\"Epoch\": e, \"Loss\": total_loss / len(train_loader), \"ValLoss\": rmse, 'optimizer': optimizer.param_groups[0]['lr'], 'scheduler': sch.get_last_lr()[0]})\n",
    "        print(f\"Epoch: {e}, Loss: {total_loss / len(train_loader)}, Val loss: {rmse}\")\n",
    "            \n",
    "        \n",
    "        if rmse < min_rmse:\n",
    "            min_rmse = rmse\n",
    "            torch.save(model.state_dict(), f'save/s4_{name}.pth')\n",
    "        \n",
    "        sch.step()\n",
    "    \n",
    "    return min_rmse\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "def validate(model, seq_len, device, val_data, testing = False, score = True):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_score = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in val_data:\n",
    "            pred_sum, pred_cnt = torch.zeros(800), torch.zeros(800)\n",
    "            valid_data = load_data(i, seq_len)  # Ensure this returns a dataset compatible with DataLoader\n",
    "            valid_loader = DataLoader(valid_data, batch_size=1000, shuffle=False)\n",
    "            \n",
    "           \n",
    "            for valid_data, valid_label, valid_padding in valid_loader:\n",
    "               \n",
    "                valid_data = valid_data.to(device)\n",
    "                data_len = len(valid_data)\n",
    "                output = model(valid_data).squeeze(2).cpu()  # Adjusted to pass only valid_data\n",
    "                # Proceed with your RMSE calculation\n",
    "\n",
    "                \n",
    "                \n",
    "                for j in range(data_len):\n",
    "                    if j < seq_len-1:\n",
    "                    \n",
    "                        pred_sum[:j+1] += output[j, -(j+1):]\n",
    "                        pred_cnt[:j+1] += 1\n",
    "                    elif j <= data_len-seq_len:\n",
    "                        pred_sum[j-seq_len+1:j+1] += output[j]\n",
    "                        pred_cnt[j-seq_len+1:j+1] += 1\n",
    "                    else:\n",
    "                        pred_sum[data_len-seq_len+1-(data_len-j):data_len-seq_len+1] += output[j, :(data_len-j)]\n",
    "                        pred_cnt[data_len-seq_len+1-(data_len-j):data_len-seq_len+1] += 1\n",
    "                truth = torch.tensor([valid_label[j,-1] for j in range(len(valid_label)-seq_len+1)], dtype=torch.float) * Rc\n",
    "                pred_sum, pred_cnt = pred_sum[:data_len-seq_len+1], pred_cnt[:data_len-seq_len+1]\n",
    "                pred = pred_sum/pred_cnt * Rc\n",
    "                \n",
    "                \n",
    "                mse = float(torch.sum(torch.pow(pred-truth, 2)))\n",
    "                rmse = math.sqrt(mse/data_len)\n",
    "                if score:\n",
    "                    sc = get_score(pred, truth)\n",
    "                    total_score += sc\n",
    "                    if testing:\n",
    "                        print(f'RMSE for {i} is {rmse}, score is {sc}')\n",
    "                        print('-'*20)\n",
    "                        \n",
    "                else:\n",
    "                    if testing:\n",
    "                    \n",
    "                        print(f'RMSE for {i} is {rmse}')\n",
    "                        print('-'*20)\n",
    "                        \n",
    "                total_loss += rmse\n",
    "        if score:\n",
    "            return total_score/len(val_data), total_loss/len(val_data)\n",
    "        else:        \n",
    "            return total_loss/len(val_data)\n",
    "                \n",
    "               \n",
    "\n",
    "train(tr, model, Loss, opt, seq_len, epochs, device, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_pred_result(data_len, out, lb):\n",
    "    pred_sum, pred_cnt = torch.zeros(800), torch.zeros(800)\n",
    "    for j in range(data_len):\n",
    "        if j < seq_len-1:\n",
    "            pred_sum[:j+1] += out[j, -(j+1):]\n",
    "            pred_cnt[:j+1] += 1\n",
    "        elif j <= data_len-seq_len:\n",
    "            pred_sum[j-seq_len+1:j+1] += out[j]\n",
    "            pred_cnt[j-seq_len+1:j+1] += 1\n",
    "        else:\n",
    "            pred_sum[data_len-seq_len+1-(data_len-j):data_len-seq_len+1] += out[j, :(data_len-j)]\n",
    "            pred_cnt[data_len-seq_len+1-(data_len-j):data_len-seq_len+1] += 1\n",
    "    truth = torch.tensor([lb[j,-1] for j in range(len(lb[0])-seq_len+1)], dtype=torch.float)\n",
    "    \n",
    "    pred_sum, pred_cnt = pred_sum[:data_len-seq_len+1], pred_cnt[:data_len-seq_len+1]\n",
    "    pred2 = pred_sum/pred_cnt\n",
    "    pred2 *= Rc\n",
    "    truth *= Rc\n",
    "    return truth, pred2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for FD001-22.txt is 17.279470053566552, score is 1096\n",
      "--------------------\n",
      "RMSE for FD001-23.txt is 9.477355712161026, score is 353\n",
      "--------------------\n",
      "RMSE for FD001-36.txt is 10.703425497807444, score is 432\n",
      "--------------------\n",
      "RMSE for FD001-8.txt is 8.268860810959149, score is 248\n",
      "--------------------\n",
      "RMSE for FD001-10.txt is 8.651956697973485, score is 382\n",
      "--------------------\n",
      "RMSE for FD001-4.txt is 9.802460084901405, score is 446\n",
      "--------------------\n",
      "RMSE for FD001-83.txt is 7.111830312611018, score is 281\n",
      "--------------------\n",
      "RMSE for FD001-86.txt is 15.243207255533807, score is 960\n",
      "--------------------\n",
      "RMSE for FD001-84.txt is 17.59441738351902, score is 1422\n",
      "--------------------\n",
      "RMSE for FD001-19.txt is 7.458868144735544, score is 194\n",
      "--------------------\n",
      "RMSE for FD001-2.txt is 5.932286297566237, score is 212\n",
      "--------------------\n",
      "RMSE for FD001-88.txt is 11.999848781922507, score is 423\n",
      "--------------------\n",
      "RMSE for FD001-56.txt is 15.865034897653993, score is 933\n",
      "--------------------\n",
      "RMSE for FD001-82.txt is 6.365475036765422, score is 206\n",
      "--------------------\n",
      "RMSE for FD001-43.txt is 11.050510882347004, score is 610\n",
      "--------------------\n",
      "RMSE for FD001-15.txt is 9.13997833863938, score is 402\n",
      "--------------------\n",
      "RMSE for FD001-33.txt is 12.858323171817242, score is 861\n",
      "--------------------\n",
      "RMSE for FD001-61.txt is 15.697410953081443, score is 1369\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(601.6666666666666, 11.138928906308982)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, seq_len, device, ts, testing=True, score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 ('rul')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba6a37c31782f78b489b06366919d70a54943a1b5ed282f036807a93db564a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
